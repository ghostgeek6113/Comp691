{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Xa8E-K5YwDqO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from skimage.feature import hog\n",
        "from skimage import color, exposure\n",
        "from sklearn.pipeline import Pipeline\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from numpy.random import RandomState\n",
        "import torchvision\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, display=True):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if display:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "          epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "          100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
        "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ],
      "metadata": {
        "id": "jJluJfR_wMUU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import random\n",
        "import torchvision\n",
        "from torchvision.models import resnet18\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "start_time = time.time()\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])\n",
        "# Data augmentation during training:\n",
        "transform_train = transforms.Compose([\n",
        "                                    transforms.RandomCrop(32, padding=2, padding_mode='reflect'),\n",
        "                                    transforms.RandomGrayscale(),\n",
        "                                    # transforms.Resize((224, 224)),\n",
        "                                    transforms.RandomHorizontalFlip(),\n",
        "                                    torchvision.transforms.RandomAffine(degrees=30),\n",
        "                                    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    normalize]) #careful to keep this one same\n",
        "transform_val = transforms.Compose([transforms.ToTensor(), normalize]) #careful to keep this one same\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "##### Cifar Data\n",
        "cifar_data = datasets.CIFAR10(root='./data',train=True, transform=transform_train, download=True)\n",
        "\n",
        "#We need two copies of this due to weird dataset api\n",
        "cifar_data_val = datasets.CIFAR10(root='./data',train=True, transform=transform_val, download=True)\n",
        "\n",
        "\n",
        "resnet_architecture_accs = []\n",
        "\n",
        "# Select random classes\n",
        "classes = random.sample(range(10), 2)\n",
        "\n",
        "for seed in range(1, 5):\n",
        "  prng = RandomState(seed)\n",
        "  random_permute = prng.permutation(np.arange(0, 1000))\n",
        "  classes =  prng.permutation(np.arange(0,10))\n",
        "\n",
        "  indx_train = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[0:25]] for classe in classes[0:2]])\n",
        "  indx_val = np.concatenate([np.where(np.array(cifar_data.targets) == classe)[0][random_permute[25:225]] for classe in classes[0:2]])\n",
        "\n",
        "  train_data = Subset(cifar_data, indx_train)\n",
        "  val_data = Subset(cifar_data_val, indx_val)\n",
        "\n",
        "  print('Num Samples For Training %d Num Samples For Val %d'%(train_data.indices.shape[0],val_data.indices.shape[0]))\n",
        "\n",
        "  train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                             batch_size=128,\n",
        "                                             shuffle=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(val_data,\n",
        "                                           batch_size=128,\n",
        "                                           shuffle=False)\n",
        "\n",
        "\n",
        "  # Define ResNet model\n",
        "  def create_resnet_model(num_classes):\n",
        "    model = resnet18(pretrained=False)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    return model\n",
        "\n",
        "  # Define hyperparameters for grid search\n",
        "  params = {\n",
        "    'lr': [0.001, 0.0001],\n",
        "    'batch_size': [4, 8],\n",
        "    'num_epochs': [5, 10]\n",
        "  }\n",
        "\n",
        "  # Initialize ResNet model\n",
        "  resnet_architecture_model = create_resnet_model(num_classes=10)\n",
        "  resnet_architecture_model = resnet_architecture_model.to(device)\n",
        "  # model.to(device)\n",
        "  optimizer = torch.optim.SGD(resnet_architecture_model.parameters(),lr=0.01, momentum=0.9,\n",
        "                              weight_decay=0.0005)\n",
        "  for epoch in range(100):\n",
        "    train(resnet_architecture_model, device, train_loader, optimizer, epoch, display=epoch%5==0)\n",
        "\n",
        "  resnet_architecture_accs.append(test(resnet_architecture_model, device, val_loader))\n",
        "\n",
        "resnet_architecture_accs = np.array(resnet_architecture_accs)\n",
        "print('Acc over 5 instances: %.2f +- %.2f'%(resnet_architecture_accs.mean(),resnet_architecture_accs.std()))\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(\"Execution time:\", execution_time, \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbtP4aS5wOml",
        "outputId": "8e3b6f00-6ec4-4487-d428-3dd5b7b3b05c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Num Samples For Training 50 Num Samples For Val 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.429064\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.717253\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.584743\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 0.495980\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.804601\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 0.481059\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.485072\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 0.553778\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.366611\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 0.429497\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 0.423182\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 0.419541\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 0.202320\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 0.522186\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 0.256521\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 0.158491\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 0.183203\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 0.199460\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 0.274679\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 0.256618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.4114, Accuracy: 275/400 (68.75%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.141577\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.641993\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.698635\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 0.537210\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.591389\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 0.603619\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.356911\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 0.368988\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.304884\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 0.303637\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 0.382197\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 0.302598\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 0.469878\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 0.280594\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 0.156310\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 0.245715\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 0.227400\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 0.084488\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 0.219385\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 0.186873\n",
            "\n",
            "Test set: Average loss: 1.4768, Accuracy: 279/400 (69.75%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 1.957307\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.663147\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.549163\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 0.397275\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.519527\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 0.353803\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.214239\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 0.194979\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.252112\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 0.140773\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 0.216327\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 0.047052\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 0.399664\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 0.129105\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 0.017826\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 0.078190\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 0.104460\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 0.344697\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 0.113769\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 0.123796\n",
            "\n",
            "Test set: Average loss: 0.7200, Accuracy: 317/400 (79.25%)\n",
            "\n",
            "Num Samples For Training 50 Num Samples For Val 400\n",
            "Train Epoch: 0 [0/50 (0%)]\tLoss: 2.668763\n",
            "Train Epoch: 5 [0/50 (0%)]\tLoss: 0.586379\n",
            "Train Epoch: 10 [0/50 (0%)]\tLoss: 0.448538\n",
            "Train Epoch: 15 [0/50 (0%)]\tLoss: 0.415217\n",
            "Train Epoch: 20 [0/50 (0%)]\tLoss: 0.420632\n",
            "Train Epoch: 25 [0/50 (0%)]\tLoss: 0.508657\n",
            "Train Epoch: 30 [0/50 (0%)]\tLoss: 0.362768\n",
            "Train Epoch: 35 [0/50 (0%)]\tLoss: 0.361229\n",
            "Train Epoch: 40 [0/50 (0%)]\tLoss: 0.601409\n",
            "Train Epoch: 45 [0/50 (0%)]\tLoss: 0.393879\n",
            "Train Epoch: 50 [0/50 (0%)]\tLoss: 0.366757\n",
            "Train Epoch: 55 [0/50 (0%)]\tLoss: 0.231137\n",
            "Train Epoch: 60 [0/50 (0%)]\tLoss: 0.230878\n",
            "Train Epoch: 65 [0/50 (0%)]\tLoss: 0.234230\n",
            "Train Epoch: 70 [0/50 (0%)]\tLoss: 0.294499\n",
            "Train Epoch: 75 [0/50 (0%)]\tLoss: 0.051197\n",
            "Train Epoch: 80 [0/50 (0%)]\tLoss: 0.074149\n",
            "Train Epoch: 85 [0/50 (0%)]\tLoss: 0.269042\n",
            "Train Epoch: 90 [0/50 (0%)]\tLoss: 0.108168\n",
            "Train Epoch: 95 [0/50 (0%)]\tLoss: 0.172657\n",
            "\n",
            "Test set: Average loss: 1.2958, Accuracy: 285/400 (71.25%)\n",
            "\n",
            "Acc over 5 instances: 72.25 +- 4.14\n",
            "Execution time: 312.4941577911377 seconds\n"
          ]
        }
      ]
    }
  ]
}